# üñ•Ô∏è Local AI Setup Guide for Windows

## Complete Guide to Run MedSigLIP + MedGemma Locally on Windows

---

## üìã System Requirements Check

### Minimum Requirements (CPU-Only Mode)
- **CPU**: Intel i5/i7 or AMD Ryzen 5/7 (4+ cores)
- **RAM**: 16GB minimum, 32GB recommended
- **Storage**: 50GB free space (for models)
- **OS**: Windows 10/11 (64-bit)
- **Python**: 3.9, 3.10, or 3.11

### Recommended Requirements (GPU Mode)
- **GPU**: NVIDIA GPU with 8GB+ VRAM
  - GTX 1070/1080 (8GB) - Basic
  - RTX 3060/3070 (8-12GB) - Good
  - RTX 3090/4090 (24GB) - Excellent
- **CPU**: Intel i7/i9 or AMD Ryzen 7/9
- **RAM**: 32GB
- **Storage**: 100GB SSD
- **CUDA**: 11.8 or 12.1

---

## üöÄ Quick Start (Choose Your Path)

### Option A: Simple Demo Mode (No GPU Required)
**Best for**: Testing the UI and workflow without real AI
- ‚úÖ No installation needed
- ‚úÖ Works on any PC
- ‚úÖ Instant setup
- ‚ùå Returns demo/mock results

### Option B: CPU-Only Mode (Slow but Works)
**Best for**: PCs without NVIDIA GPU
- ‚úÖ Works on any PC
- ‚úÖ Real AI models
- ‚ùå Very slow (30-60 seconds per image)

### Option C: GPU Mode (Recommended)
**Best for**: PCs with NVIDIA GPU
- ‚úÖ Fast inference (1-5 seconds)
- ‚úÖ Real AI models
- ‚úÖ Production-ready
- ‚ùå Requires NVIDIA GPU

---

## üì¶ Step 1: Install Prerequisites

### 1.1 Install Python

```powershell
# Check if Python is installed
python --version

# If not installed, download from:
# https://www.python.org/downloads/
# Choose Python 3.10.x (recommended)
# ‚úÖ Check "Add Python to PATH" during installation
```

### 1.2 Install Git (Optional but Recommended)

```powershell
# Download from: https://git-scm.com/download/win
# Or use winget:
winget install Git.Git
```

### 1.3 Install CUDA Toolkit (GPU Mode Only)

```powershell
# Check if you have NVIDIA GPU
# Open Device Manager ‚Üí Display adapters

# Download CUDA Toolkit 11.8:
# https://developer.nvidia.com/cuda-11-8-0-download-archive
# Choose: Windows ‚Üí x86_64 ‚Üí 10/11 ‚Üí exe (local)

# After installation, verify:
nvcc --version
nvidia-smi
```

---

## üõ†Ô∏è Step 2: Setup AI Services

### Create Project Structure

```powershell
# Navigate to your project
cd G:\RADIOLOGY\redio-test

# Create AI services directory
mkdir ai-services
cd ai-services
```

### 2.1 Create Virtual Environment

```powershell
# Create virtual environment
python -m venv venv

# Activate it
.\venv\Scripts\Activate.ps1

# If you get execution policy error:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Upgrade pip
python -m pip install --upgrade pip
```

### 2.2 Install Dependencies

```powershell
# Create requirements.txt
@"
# Core dependencies
torch==2.1.0
torchvision==0.16.0
transformers==4.35.0
accelerate==0.24.0
flask==3.0.0
pillow==10.1.0
numpy==1.24.3
requests==2.31.0

# Optional: For GPU optimization
bitsandbytes==0.41.1

# Optional: For better performance
einops==0.7.0
safetensors==0.4.0
"@ | Out-File -FilePath requirements.txt -Encoding utf8

# Install for CPU
pip install -r requirements.txt

# OR Install for GPU (CUDA 11.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

### 2.3 Verify Installation

```powershell
# Test PyTorch
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"

# Expected output:
# PyTorch: 2.1.0
# CUDA Available: True  (if GPU) or False (if CPU)
```

---

## üì• Step 3: Download AI Models

### Option A: Use Lightweight Demo Models (Recommended for Testing)

```powershell
# Create models directory
mkdir models
cd models

# We'll use smaller, publicly available models for demo
# These are NOT the actual MedSigLIP/MedGemma but work similarly
```

### Option B: Download Full Models (Requires Hugging Face Account)

```powershell
# Install Hugging Face CLI
pip install huggingface-hub

# Login to Hugging Face
huggingface-cli login
# Enter your token from: https://huggingface.co/settings/tokens

# Download models (this will take time - models are large!)
# Note: Replace with actual model names when available
huggingface-cli download microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 --local-dir ./medsigclip
```

---

## üêç Step 4: Create AI Service Scripts

### 4.1 Create MedSigLIP Server (Simple Version)

Create `ai-services/medsigclip_server.py`:

```python
"""
MedSigLIP Server - Simplified Local Version
Medical image classification service
"""

from flask import Flask, request, jsonify
from PIL import Image
import torch
import io
import base64
import time
import os
import sys

app = Flask(__name__)

# Configuration
PORT = int(os.getenv('PORT', 5001))
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
DEMO_MODE = os.getenv('DEMO_MODE', 'true').lower() == 'true'

print(f"üöÄ Starting MedSigLIP Server")
print(f"   Device: {DEVICE}")
print(f"   Demo Mode: {DEMO_MODE}")
print(f"   Port: {PORT}")

# Load model (or use demo mode)
model = None
if not DEMO_MODE:
    try:
        print("Loading model...")
        # TODO: Load actual model when available
        # from transformers import AutoModel, AutoProcessor
        # model = AutoModel.from_pretrained('./models/medsigclip')
        print("‚ö†Ô∏è  Real model not available, using demo mode")
        DEMO_MODE = True
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to load model: {e}")
        print("   Falling back to demo mode")
        DEMO_MODE = True

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status': 'healthy',
        'model': 'MedSigLIP-0.4B (Demo)' if DEMO_MODE else 'MedSigLIP-0.4B',
        'device': DEVICE,
        'gpu_available': torch.cuda.is_available(),
        'demo_mode': DEMO_MODE
    })

@app.route('/classify', methods=['POST'])
def classify():
    try:
        data = request.json
        image_b64 = data.get('image')
        modality = data.get('modality', 'unknown')
        
        # Decode image
        image_bytes = base64.b64decode(image_b64)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        start_time = time.time()
        
        if DEMO_MODE:
            # Demo mode - return mock results
            time.sleep(0.2)  # Simulate processing
            
            # Simple heuristic based on image properties
            width, height = image.size
            avg_brightness = sum(image.convert('L').getdata()) / (width * height)
            
            # Mock classification based on modality
            classifications = {
                'XR': ['normal', 'fracture', 'pneumonia', 'effusion'],
                'CT': ['normal', 'mass', 'hemorrhage', 'infarct'],
                'MR': ['normal', 'tumor', 'edema', 'lesion'],
                'US': ['normal', 'cyst', 'mass', 'fluid']
            }
            
            labels = classifications.get(modality, ['normal', 'abnormal'])
            classification = labels[0] if avg_brightness > 128 else labels[1]
            confidence = 0.75 + (avg_brightness / 1000)
            
            features = [float(i) for i in range(512)]  # Mock features
            
        else:
            # Real model inference
            # TODO: Implement actual model inference
            classification = 'normal'
            confidence = 0.85
            features = [0.0] * 512
        
        processing_time = time.time() - start_time
        
        return jsonify({
            'classification': classification,
            'confidence': float(confidence),
            'features': features,
            'processing_time': processing_time,
            'modality': modality,
            'demo_mode': DEMO_MODE
        })
        
    except Exception as e:
        print(f"Error in classify: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print(f"\n‚úÖ MedSigLIP Server running on http://localhost:{PORT}")
    print(f"   Test with: curl http://localhost:{PORT}/health\n")
    app.run(host='0.0.0.0', port=PORT, debug=False)
```

### 4.2 Create MedGemma Server (Simple Version)

Create `ai-services/medgemma_server.py`:

```python
"""
MedGemma Server - Simplified Local Version
Radiology report generation service
"""

from flask import Flask, request, jsonify
from PIL import Image
import torch
import io
import base64
import time
import os
import random

app = Flask(__name__)

# Configuration
PORT = int(os.getenv('PORT', 5002))
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
DEMO_MODE = os.getenv('DEMO_MODE', 'true').lower() == 'true'

print(f"üöÄ Starting MedGemma Server")
print(f"   Device: {DEVICE}")
print(f"   Demo Mode: {DEMO_MODE}")
print(f"   Port: {PORT}")

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status': 'healthy',
        'model': 'MedGemma-4B (Demo)' if DEMO_MODE else 'MedGemma-4B',
        'device': DEVICE,
        'gpu_available': torch.cuda.is_available(),
        'demo_mode': DEMO_MODE
    })

@app.route('/generate-report', methods=['POST'])
def generate_report():
    try:
        data = request.json
        image_b64 = data.get('image')
        modality = data.get('modality', 'unknown')
        patient_context = data.get('patientContext', {})
        
        # Decode image
        image_bytes = base64.b64decode(image_b64)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        start_time = time.time()
        
        if DEMO_MODE:
            # Demo mode - generate template report
            time.sleep(1.0)  # Simulate processing
            
            age = patient_context.get('age', 'unknown')
            sex = patient_context.get('sex', 'unknown')
            history = patient_context.get('clinicalHistory', 'not provided')
            
            findings = f"""TECHNIQUE:
{modality} imaging was performed according to standard protocol.

FINDINGS:
This is a demonstration report generated in demo mode. The AI models (MedSigLIP and MedGemma) are not currently running with real weights.

Image analysis shows:
- Image dimensions: {image.size[0]}x{image.size[1]} pixels
- Patient age: {age} years
- Patient sex: {sex}
- Clinical history: {history}

To enable real AI-powered report generation:
1. Download the actual MedGemma-4B model weights
2. Set DEMO_MODE=false in environment
3. Restart the AI services

For now, this demo report demonstrates the interface and workflow."""

            impression = f"""IMPRESSION:
Demo mode active - AI models not loaded with real weights.

This placeholder report shows how the AI-generated reports will appear in the interface.

Clinical correlation and radiologist review required."""

            recommendations = [
                "Enable real AI models for actual analysis",
                "Clinical correlation recommended",
                "Radiologist review required"
            ]
            
        else:
            # Real model inference
            # TODO: Implement actual model inference
            findings = "Real model findings would appear here"
            impression = "Real model impression would appear here"
            recommendations = []
        
        processing_time = time.time() - start_time
        
        return jsonify({
            'findings': findings,
            'impression': impression,
            'recommendations': recommendations,
            'processing_time': processing_time,
            'confidence': 0.80,
            'demo_mode': DEMO_MODE
        })
        
    except Exception as e:
        print(f"Error in generate_report: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print(f"\n‚úÖ MedGemma Server running on http://localhost:{PORT}")
    print(f"   Test with: curl http://localhost:{PORT}/health\n")
    app.run(host='0.0.0.0', port=PORT, debug=False)
```

---

## üöÄ Step 5: Start AI Services

### 5.1 Create Start Scripts

Create `ai-services/start-ai-services.bat`:

```batch
@echo off
echo ========================================
echo Starting Medical AI Services (Local)
echo ========================================
echo.

REM Activate virtual environment
call venv\Scripts\activate.bat

REM Set demo mode (change to false when models are ready)
set DEMO_MODE=true

echo Starting MedSigLIP Server (Port 5001)...
start "MedSigLIP" cmd /k "python medsigclip_server.py"

timeout /t 3

echo Starting MedGemma Server (Port 5002)...
start "MedGemma" cmd /k "python medgemma_server.py"

echo.
echo ========================================
echo AI Services Started!
echo ========================================
echo.
echo MedSigLIP: http://localhost:5001
echo MedGemma:  http://localhost:5002
echo.
echo Press any key to test services...
pause

REM Test services
echo.
echo Testing MedSigLIP...
curl http://localhost:5001/health

echo.
echo Testing MedGemma...
curl http://localhost:5002/health

echo.
echo ========================================
echo Setup Complete!
echo ========================================
pause
```

Create `ai-services/stop-ai-services.bat`:

```batch
@echo off
echo Stopping AI Services...

REM Kill Python processes running on ports 5001 and 5002
for /f "tokens=5" %%a in ('netstat -aon ^| find ":5001" ^| find "LISTENING"') do taskkill /F /PID %%a
for /f "tokens=5" %%a in ('netstat -aon ^| find ":5002" ^| find "LISTENING"') do taskkill /F /PID %%a

echo AI Services stopped.
pause
```

### 5.2 Start the Services

```powershell
# Navigate to ai-services directory
cd G:\RADIOLOGY\redio-test\ai-services

# Run the start script
.\start-ai-services.bat
```

---

## ‚öôÔ∏è Step 6: Configure Your Application

### Update `server/.env`

```env
# AI Service Configuration (Local)
MEDSIGCLIP_API_URL=http://localhost:5001
MEDSIGCLIP_ENABLED=true

MEDGEMMA_4B_API_URL=http://localhost:5002
MEDGEMMA_4B_ENABLED=true

MEDGEMMA_27B_API_URL=http://localhost:5003
MEDGEMMA_27B_ENABLED=false

# Timeouts (increase for CPU mode)
AI_REQUEST_TIMEOUT=60000
AI_CLASSIFICATION_TIMEOUT=30000
AI_REPORT_TIMEOUT=60000
```

### Restart Your Backend Server

```powershell
# Navigate to server directory
cd G:\RADIOLOGY\redio-test\server

# Restart server
npm restart
```

---

## üß™ Step 7: Test the Integration

### 7.1 Test AI Services Directly

```powershell
# Test MedSigLIP health
curl http://localhost:5001/health

# Test MedGemma health
curl http://localhost:5002/health
```

### 7.2 Test from Your Application

```powershell
# Start your backend (if not running)
cd G:\RADIOLOGY\redio-test\server
npm start

# Start your frontend (in another terminal)
cd G:\RADIOLOGY\redio-test\viewer
npm run dev
```

### 7.3 Test in Browser

1. Open `http://localhost:5173`
2. Login to your application
3. Upload a DICOM study
4. Open the viewer
5. Look for AI Analysis panel
6. Click "Analyze with AI"
7. You should see demo results!

---

## üìä Performance Expectations

### Demo Mode (Current Setup)
- Classification: ~200ms
- Report Generation: ~1 second
- ‚úÖ Works on any PC
- ‚úÖ No GPU required
- ‚ùå Mock results only

### CPU Mode (With Real Models)
- Classification: 5-10 seconds
- Report Generation: 30-60 seconds
- ‚úÖ Real AI results
- ‚ùå Very slow

### GPU Mode (With Real Models)
- Classification: 100-500ms
- Report Generation: 3-8 seconds
- ‚úÖ Real AI results
- ‚úÖ Fast enough for production

---

## üîß Troubleshooting

### Port Already in Use

```powershell
# Find what's using port 5001
netstat -ano | findstr :5001

# Kill the process
taskkill /F /PID <PID>
```

### Python Not Found

```powershell
# Add Python to PATH
# Control Panel ‚Üí System ‚Üí Advanced ‚Üí Environment Variables
# Add: C:\Users\<YourUser>\AppData\Local\Programs\Python\Python310
```

### CUDA Out of Memory

```powershell
# Reduce batch size or use CPU mode
set DEVICE=cpu
```

### Module Not Found

```powershell
# Reinstall dependencies
cd ai-services
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

---

## üéØ Next Steps

### Current Status: Demo Mode ‚úÖ
- ‚úÖ AI services running locally
- ‚úÖ Returns demo/mock results
- ‚úÖ UI integration working
- ‚úÖ Workflow tested

### To Enable Real AI:

1. **Download Real Models**:
   ```powershell
   # When models become available
   huggingface-cli download <model-name> --local-dir ./models
   ```

2. **Update Scripts**:
   - Uncomment model loading code
   - Set `DEMO_MODE=false`

3. **Restart Services**:
   ```powershell
   .\stop-ai-services.bat
   .\start-ai-services.bat
   ```

---

## üí° Tips

### Run on Startup

Create a shortcut to `start-ai-services.bat` in:
```
C:\Users\<YourUser>\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup
```

### Monitor Performance

```powershell
# Watch GPU usage (if available)
nvidia-smi -l 1

# Watch CPU/RAM usage
# Open Task Manager ‚Üí Performance
```

### Logs

```powershell
# View logs in the command windows
# Or redirect to file:
python medsigclip_server.py > medsigclip.log 2>&1
```

---

## üìö Quick Reference

### Start Services
```powershell
cd G:\RADIOLOGY\redio-test\ai-services
.\start-ai-services.bat
```

### Stop Services
```powershell
.\stop-ai-services.bat
```

### Test Services
```powershell
curl http://localhost:5001/health
curl http://localhost:5002/health
```

### View Logs
- Check the command windows that opened
- Or check `medsigclip.log` and `medgemma.log`

---

**Status**: ‚úÖ Ready for Local Demo Mode
**Next**: Download real models for production use
